{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier in PySpark - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "\n",
    "In this lab, you will build a Random Forest Classifier model to study the ecommerce behavior of consumers from a multi-category store. First, you will need to download the data to your local machine, then you will load the data from the local machine onto a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives  \n",
    "\n",
    "* Use the kaggle eCommerce dataset in PySpark\n",
    "* Build and train a random forest classifier in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "* Accept the Kaggle policy and download the data from [Kaggle](https://www.kaggle.com/code/tshephisho/ecommerce-behaviour-using-xgboost/data)\n",
    "* For the first model you will only use the 2019-Nov csv data (which is still around ~2gb zipped)\n",
    "* You will run this notebook in a new `pyspark-env` environment following [these setup instructions without docker](https://github.com/learn-co-curriculum/dsc-spark-docker-installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d3c0685e46fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# instantiate spark instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m spark = (\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Random Forest eCommerce\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.executor.memory\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"4g\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.driver.memory\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"4g\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession  # entry point for pyspark\n",
    "\n",
    "# instantiate spark instance\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Random Forest eCommerce\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../archive/2019-Nov.csv\"  # wherever path you saved the kaggle file to\n",
    "df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "df.printSchema()  # to see the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use Pandas to explore the dataset instead of Pyspark, you have to use the `action` functions, which then means there will be a network shuffle. For smaller dataset such as the Iris dataset which is about ~1KB this is no problem. The current dataset may be too large, and may throw an `OutOfMemory` error if you attempt to load the data into a Pandas dataframe. You should only take a few rows for exploratory analysis if you are more comfortable with Pandas. Otherwise, stick with native PySpark functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>event_time</th>\n",
       "      <td>2019-11-01 00:00:00 UTC</td>\n",
       "      <td>2019-11-01 00:00:00 UTC</td>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>2019-11-01 00:00:02 UTC</td>\n",
       "      <td>2019-11-01 00:00:02 UTC</td>\n",
       "      <td>2019-11-01 00:00:02 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type</th>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_id</th>\n",
       "      <td>1003461</td>\n",
       "      <td>5000088</td>\n",
       "      <td>17302664</td>\n",
       "      <td>3601530</td>\n",
       "      <td>1004775</td>\n",
       "      <td>1306894</td>\n",
       "      <td>1306421</td>\n",
       "      <td>15900065</td>\n",
       "      <td>12708937</td>\n",
       "      <td>1004258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_id</th>\n",
       "      <td>2053013555631882655</td>\n",
       "      <td>2053013566100866035</td>\n",
       "      <td>2053013553853497655</td>\n",
       "      <td>2053013563810775923</td>\n",
       "      <td>2053013555631882655</td>\n",
       "      <td>2053013558920217191</td>\n",
       "      <td>2053013558920217191</td>\n",
       "      <td>2053013558190408249</td>\n",
       "      <td>2053013553559896355</td>\n",
       "      <td>2053013555631882655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_code</th>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>appliances.sewing_machine</td>\n",
       "      <td>None</td>\n",
       "      <td>appliances.kitchen.washer</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>computers.notebook</td>\n",
       "      <td>computers.notebook</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>xiaomi</td>\n",
       "      <td>janome</td>\n",
       "      <td>creed</td>\n",
       "      <td>lg</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>hp</td>\n",
       "      <td>hp</td>\n",
       "      <td>rondell</td>\n",
       "      <td>michelin</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>489.07</td>\n",
       "      <td>293.65</td>\n",
       "      <td>28.31</td>\n",
       "      <td>712.87</td>\n",
       "      <td>183.27</td>\n",
       "      <td>360.09</td>\n",
       "      <td>514.56</td>\n",
       "      <td>30.86</td>\n",
       "      <td>72.72</td>\n",
       "      <td>732.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>520088904</td>\n",
       "      <td>530496790</td>\n",
       "      <td>561587266</td>\n",
       "      <td>518085591</td>\n",
       "      <td>558856683</td>\n",
       "      <td>520772685</td>\n",
       "      <td>514028527</td>\n",
       "      <td>518574284</td>\n",
       "      <td>532364121</td>\n",
       "      <td>532647354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_session</th>\n",
       "      <td>4d3b30da-a5e4-49df-b1a8-ba5943f1dd33</td>\n",
       "      <td>8e5f4f83-366c-4f70-860e-ca7417414283</td>\n",
       "      <td>755422e7-9040-477b-9bd2-6a6e8fd97387</td>\n",
       "      <td>3bfb58cd-7892-48cc-8020-2f17e6de6e7f</td>\n",
       "      <td>313628f1-68b8-460d-84f6-cec7a8796ef2</td>\n",
       "      <td>816a59f3-f5ae-4ccd-9b23-82aa8c23d33c</td>\n",
       "      <td>df8184cc-3694-4549-8c8c-6b5171877376</td>\n",
       "      <td>5e6ef132-4d7c-4730-8c7f-85aa4082588f</td>\n",
       "      <td>0a899268-31eb-46de-898d-09b2da950b24</td>\n",
       "      <td>d2d3d2c6-631d-489e-9fb5-06f340b85be0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0  \\\n",
       "event_time                  2019-11-01 00:00:00 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  1003461   \n",
       "category_id                     2053013555631882655   \n",
       "category_code                electronics.smartphone   \n",
       "brand                                        xiaomi   \n",
       "price                                        489.07   \n",
       "user_id                                   520088904   \n",
       "user_session   4d3b30da-a5e4-49df-b1a8-ba5943f1dd33   \n",
       "\n",
       "                                                  1  \\\n",
       "event_time                  2019-11-01 00:00:00 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  5000088   \n",
       "category_id                     2053013566100866035   \n",
       "category_code             appliances.sewing_machine   \n",
       "brand                                        janome   \n",
       "price                                        293.65   \n",
       "user_id                                   530496790   \n",
       "user_session   8e5f4f83-366c-4f70-860e-ca7417414283   \n",
       "\n",
       "                                                  2  \\\n",
       "event_time                  2019-11-01 00:00:01 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                 17302664   \n",
       "category_id                     2053013553853497655   \n",
       "category_code                                  None   \n",
       "brand                                         creed   \n",
       "price                                         28.31   \n",
       "user_id                                   561587266   \n",
       "user_session   755422e7-9040-477b-9bd2-6a6e8fd97387   \n",
       "\n",
       "                                                  3  \\\n",
       "event_time                  2019-11-01 00:00:01 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  3601530   \n",
       "category_id                     2053013563810775923   \n",
       "category_code             appliances.kitchen.washer   \n",
       "brand                                            lg   \n",
       "price                                        712.87   \n",
       "user_id                                   518085591   \n",
       "user_session   3bfb58cd-7892-48cc-8020-2f17e6de6e7f   \n",
       "\n",
       "                                                  4  \\\n",
       "event_time                  2019-11-01 00:00:01 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  1004775   \n",
       "category_id                     2053013555631882655   \n",
       "category_code                electronics.smartphone   \n",
       "brand                                        xiaomi   \n",
       "price                                        183.27   \n",
       "user_id                                   558856683   \n",
       "user_session   313628f1-68b8-460d-84f6-cec7a8796ef2   \n",
       "\n",
       "                                                  5  \\\n",
       "event_time                  2019-11-01 00:00:01 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  1306894   \n",
       "category_id                     2053013558920217191   \n",
       "category_code                    computers.notebook   \n",
       "brand                                            hp   \n",
       "price                                        360.09   \n",
       "user_id                                   520772685   \n",
       "user_session   816a59f3-f5ae-4ccd-9b23-82aa8c23d33c   \n",
       "\n",
       "                                                  6  \\\n",
       "event_time                  2019-11-01 00:00:01 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  1306421   \n",
       "category_id                     2053013558920217191   \n",
       "category_code                    computers.notebook   \n",
       "brand                                            hp   \n",
       "price                                        514.56   \n",
       "user_id                                   514028527   \n",
       "user_session   df8184cc-3694-4549-8c8c-6b5171877376   \n",
       "\n",
       "                                                  7  \\\n",
       "event_time                  2019-11-01 00:00:02 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                 15900065   \n",
       "category_id                     2053013558190408249   \n",
       "category_code                                  None   \n",
       "brand                                       rondell   \n",
       "price                                         30.86   \n",
       "user_id                                   518574284   \n",
       "user_session   5e6ef132-4d7c-4730-8c7f-85aa4082588f   \n",
       "\n",
       "                                                  8  \\\n",
       "event_time                  2019-11-01 00:00:02 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                 12708937   \n",
       "category_id                     2053013553559896355   \n",
       "category_code                                  None   \n",
       "brand                                      michelin   \n",
       "price                                         72.72   \n",
       "user_id                                   532364121   \n",
       "user_session   0a899268-31eb-46de-898d-09b2da950b24   \n",
       "\n",
       "                                                  9  \n",
       "event_time                  2019-11-01 00:00:02 UTC  \n",
       "event_type                                     view  \n",
       "product_id                                  1004258  \n",
       "category_id                     2053013555631882655  \n",
       "category_code                electronics.smartphone  \n",
       "brand                                         apple  \n",
       "price                                        732.07  \n",
       "user_id                                   532647354  \n",
       "user_session   d2d3d2c6-631d-489e-9fb5-06f340b85be0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.take(10), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Know your Customers\n",
    "\n",
    "How many unique customers visit the site?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT user_id)|\n",
      "+-----------------------+\n",
      "|                3696117|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using native pyspark\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct(\"user_id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice the spark progress bar when you triggered the `action` function? The `show()` function is the `action` function which means the lazy evaluation of Spark was triggered and completed a certain job. `read.csv` should have been another job. If you go to `localhost:4040` you should be able to see 2 completed jobs under the `Jobs` tab, which are `csv` and `showString`. While a heavy job is getting executed, you can take a look at the `Executors` tab to examine the executors completing the tasks in parellel. Now, you may not see if we run this on a local machine, but this behavior should definitely be visible if you're on a cloud system, such as EMR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Visitors Daily Trend\n",
    "\n",
    "Does traffic flunctuate by date? Try using the event_time to see traffic, and draw the plots for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for event_time you should use a window and groupby a time period\n",
    "from pyspark.sql.functions import window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: You would still like to see the cart abandonment rate using the dataset. What relevant features can we use for modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will start building the model. Add the columns you would like to use for predictor features in the model to the `feature_cols` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = []  # columns you'd like to use\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a string column, you can use the `StringIndexer` to encode the column. Update the `inputCol` keyword argument so that you can encode the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labeler = StringIndexer(\n",
    "    inputCol=\"\", outputCol=\"encoded\"\n",
    ")  # what should we use for the inputCol here?\n",
    "df = labeler.fit(df).transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the train/test dataset with a 70/30 `randomSplit` and a random seed set to 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit()\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to add in the name of the feature column and the name of the `labelCol` you previously encoded for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"\", labelCol=\"\")\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)\n",
    "# what goes in the select() function?\n",
    "predictions.select().show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job execution is done, evaluate the model's performance. Add in the `labelCol` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Use the confusion matrix to see the other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "preds_and_labels = (\n",
    "    predictions.select([\"prediction\", \"encoded\"])\n",
    "    .withColumn(\"encoded\", F.col(\"encoded\").cast(FloatType()))\n",
    "    .orderBy(\"prediction\")\n",
    ")\n",
    "preds_and_labels = preds_and_labels.select([\"prediction\", \"encoded\"])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
